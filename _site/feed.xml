<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Anand's Blog</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml" />
<link rel="alternate" type="text/html" href="http://localhost:4000" />
<updated>2018-02-11T14:56:39+05:30</updated>
<id>http://localhost:4000/</id>
<author>
  <name>Anand Nautiyal</name>
  <uri>http://localhost:4000/</uri>
  <email>mail@indrajith.me</email>
</author>


<entry>
  <title type="html"><![CDATA[An E-commerce application for aftermarket auto parts]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/blog/An_e_commerce_application/" />
  <id>http://localhost:4000/blog/An_e_commerce_application</id>
  <published>2017-12-29T11:58:29+05:30</published>
  <updated>2017-12-29T11:58:29+05:30</updated>
  <author>
    <name>Anand Nautiyal</name>
    <uri>http://localhost:4000</uri>
    <email>mail@indrajith.me</email>
  </author>
  <content type="html">
    &lt;h1 id=&quot;auto-parts-shop---an-e-commerce-application&quot;&gt;Auto Parts Shop - An E-commerce application&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/data-engineering.png&quot; alt=&quot;DataEngineering&quot; title=&quot;DataEngineering&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#requirements&quot;&gt;Requirements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#apachespark&quot;&gt;Apache_Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Auto Parts Application is an an E-commerce project which deals in plethora of Aftermarket products of several vehicles. It is huge project with several teams working on it. We are supplied with raw data in various formats and we have to analyse it and make it into the desired forms.
It can involve working on Apache Spark, Scala or working it out in Python.&lt;/p&gt;

&lt;h2 id=&quot;requirements&quot;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Spark (2.1.0)&lt;/li&gt;
  &lt;li&gt;Java (1.8)&lt;/li&gt;
  &lt;li&gt;Scala (2.11)&lt;/li&gt;
  &lt;li&gt;Sbt (0.13.1)&lt;/li&gt;
  &lt;li&gt;RAM (&amp;gt;= 8 GB)&lt;/li&gt;
  &lt;li&gt;Processor (&amp;gt;= 2 Ghz)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get update&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install default-jdk&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install scala&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install git&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tar xvf spark-2.0.2-bin-hadoop2.7.tgz&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd spark-2.0.2-bin-hadoop2.7.tgz&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd bin&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;./spark-shell&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apachespark&quot;&gt;Apache_Spark&lt;/h2&gt;

&lt;p&gt;If the present computing is to be analysed, data computing stands out of the crowd. Data lies at the core of the Internet era. Alongwith the product itself, a business relies a lot on the analysis of the data trends to make the product better and more wanted. And, it is a magical world out there when you see data talking to you and telling you the obvious.&lt;/p&gt;

&lt;p&gt;There are several tools out there for data computing. However, Apache Spark has gained a lot of popularity because of its ease to work with. It makes thing easier than hadoop. The major flaw with hadoop was the hectic process of writing out dataframes frequently after applying an operation on it. While, in spark, a series of operations can be performed on the dataframes and writing it out at the end of all the operations.&lt;/p&gt;

&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;Our project involves raw data files which are unstructured. They contain a lot of scattered information which needs to be mined out for describing our product better. A single sku carries plenty of information in the form of files. All this information needs to be fetched from these files by applying various operations, as deemed necessary. The raw data, once processed acts as the source of information about that sku when it is viewed online.&lt;/p&gt;

&lt;p&gt;The major task is to maintain the catalogue. The catalogue once live, needs to be analysed based on the sale trend. Then recommendation system sets it foot in. Recommendation models are used on the fetched data and better recommendations are provided to the client.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/blog/An_e_commerce_application/&quot;&gt;An E-commerce application for aftermarket auto parts&lt;/a&gt; was originally published by Anand Nautiyal at &lt;a href=&quot;http://localhost:4000&quot;&gt;Anand's Blog&lt;/a&gt; on December 29, 2017.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Time efficient Disvovery of moving object groups from Trajectory data]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/blog/Time_efficient_Disvovery_of_-moving_object_groups_from_Trajectory_data/" />
  <id>http://localhost:4000/blog/Time_efficient_Disvovery_of_ moving_object_groups_from_Trajectory_data</id>
  <published>2017-07-29T11:58:29+05:30</published>
  <updated>2017-07-29T11:58:29+05:30</updated>
  <author>
    <name>Anand Nautiyal</name>
    <uri>http://localhost:4000</uri>
    <email>mail@indrajith.me</email>
  </author>
  <content type="html">
    &lt;h1 id=&quot;travelling-companion-problem&quot;&gt;Travelling Companion Problem&lt;/h1&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#requirements&quot;&gt;Requirements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#relatedwork&quot;&gt;Related_Work&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#problemdefinition&quot;&gt;Problem_Definition&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#algorithms&quot;&gt;Algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experiments&quot;&gt;Experiments&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results&quot;&gt;Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Travelling companion is a type of object group, like swarm, convoy, flock, etc. As the objects move with respect to time, they leave their traces in the form of GPS locations. This gives rise to humongous spatio-temporal data which can be used in several domains of study like, social network applications, carpooling, scientific study of migratory birds, etc. The traditional DBSCAN algorithm as used in the Travelling Companion study has O(n^2) time complexity, which can be futile for streaming data. Our approach improved the time complexity to O(n log n), which is a huge improvement.&lt;/p&gt;

&lt;h2 id=&quot;requirements&quot;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Netbeans (8.1)&lt;/li&gt;
  &lt;li&gt;Java (1.8)&lt;/li&gt;
  &lt;li&gt;RAM (&amp;gt;= 4 GB)&lt;/li&gt;
  &lt;li&gt;Processor (&amp;gt;= 2 Ghz)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;relatedwork&quot;&gt;Related_Work&lt;/h1&gt;

&lt;p&gt;There have been many similar studies to cluster moving together object groups. Some of them are flock, convoy, gathering, swarm, etc. Flock 
discovers a group of objects which are together for ‘k’ consecutive timestamps inside a disc of radius ‘r’. Convoy uses the density connectedness and has no restriction on shape and size. Swarm relaxes the constraint of clustering objects in consecutive timestamps. It can have non-consecutive timestamps. Gathering aims to find the coming together of certain objects for a particular time period.&lt;/p&gt;

&lt;h1 id=&quot;problemdefinition&quot;&gt;Problem_Definition&lt;/h1&gt;

&lt;p&gt;A sequence of snapshots s1,…,sn represent the trajectory data stream. Each snapshot has the spatial information of its constituent objects in the form of their latitudes and longitudes at a particular time. A snapshot can be represented as
 s_i = {o_1, x_(1,i) , y_(1,i)},…, {o_n, x_(n,i), y_(n,i)},
 where x_(j,i) , y_(j,i) are the spatial coordinates of object o_j at snapshot s_i.&lt;/p&gt;

&lt;p&gt;As soon as a snapshot arrives, the objective is to find a traveling companion.&lt;/p&gt;

&lt;p&gt;Definition 1 (Traveling Companion) Let S’ represents the size threshold and T’ represents the duration threshold, an object set ‘q’ is a traveling companion if:&lt;/p&gt;

&lt;p&gt;(1) The constituents of ‘q’ are density-connected to each other for a period ‘t’ where t ≥ T’;
(2) S’(q) ≥ S’.&lt;/p&gt;

&lt;h3 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h3&gt;

&lt;p&gt;Here is the Algorithmic representation of the work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Algo1.png&quot; alt=&quot;Algorithm&quot; title=&quot;Algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/Algo2.png&quot; alt=&quot;Algorithm&quot; title=&quot;Algorithm&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;The experiments are performed on the Microsoft T-drive Taxi [9] dataset for a different number of objects. Trajectories were generated up to 5000 taxis. Three datasets D1 (500 objects), D2 (1000 objects) and D3 (5000 objects) are used for performance evaluation. The grid-based approach outperforms the buddy based companion discovery and shows very promising results with an order of reduction in time.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/Results.png&quot; alt=&quot;Results&quot; title=&quot;Results&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This work incorporates the grid-based clustering approach with the buddy-based companion discovery algorithm for finding traveling companions in streaming data. The O(n2) time complexity of clustering step in the traveling companion discovery is reduced to O(nlogn) by using the grid-based approach. The efficiency of the grid-based approach is evident by the experimental results on different datasets of varying size. This reduction in time is highly significant to meet the requirements of real life applications.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/blog/Time_efficient_Disvovery_of_-moving_object_groups_from_Trajectory_data/&quot;&gt;Time efficient Disvovery of moving object groups from Trajectory data&lt;/a&gt; was originally published by Anand Nautiyal at &lt;a href=&quot;http://localhost:4000&quot;&gt;Anand's Blog&lt;/a&gt; on July 29, 2017.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Machine Learning Algorithms for Recommender System - a comparative analysis]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/Machine_Learning_Algorithms_for_Recommender_System_a_comparative_analysis/" />
  <id>http://localhost:4000/Machine_Learning_Algorithms_for_Recommender_System_a_comparative_analysis</id>
  <published>2017-04-15T18:54:48+05:30</published>
  <updated>2017-04-15T18:54:48+05:30</updated>
  <author>
    <name>Anand Nautiyal</name>
    <uri>http://localhost:4000</uri>
    <email>mail@indrajith.me</email>
  </author>
  <content type="html">
    &lt;h1 id=&quot;a-thorough-analysis-of-the-machine-learning-algorithms&quot;&gt;A thorough analysis of the Machine Learning Algorithms&lt;/h1&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#requirements&quot;&gt;Requirements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#algorithms&quot;&gt;Algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results&quot;&gt;Results&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recommendation system is one of the most popular applications of Artificial Intelligence which attracts many researchers all over the globe. The advent of the Internet era has brought wide implementation of recommendation system in our everyday lives. There are many machine learning techniques which can be used to realize the recommendation system. Among all these techniques we are dealing with Content Based Filtering, Collaborative Based Filtering, Hybrid Content-Collaborative Based Filtering, k-mean clustering and Naive Bayes classifier. We have exploited these algorithms to their extreme in order to achieve the best possible precision and have presented a comprehensive comparative analysis. The strength of all these algorithms can be clearly realized by the significant enhancement in the accuracy, depicted by the experimental analysis taking cold start problem into consideration.&lt;/p&gt;

&lt;h2 id=&quot;requirements&quot;&gt;Requirements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Netbeans (8.1)&lt;/li&gt;
  &lt;li&gt;Java (1.8)&lt;/li&gt;
  &lt;li&gt;RAM (&amp;gt;= 4 GB)&lt;/li&gt;
  &lt;li&gt;Processor (&amp;gt;= 2 Ghz)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;algorithms&quot;&gt;Algorithms&lt;/h3&gt;

&lt;p&gt;1) Content Based Filtering&lt;/p&gt;

&lt;p&gt;The Content Based Filtering considers the items rated by a user to formulate the future recommendations while exploring the internet services. A user tends to rate an item which he likes or dislikes. His ratings reflect his response towards that item. If he likes an item, he rates it higher and lesser ratings denote that he is not much interested. These rated items serve as the ‘content’ in the Content Based Filtering. Based on this content, the user is recommended future items which he might approve of. Here, the user is recommended movies which fall in a particular
genre of his liking.&lt;/p&gt;

&lt;p&gt;Following Algorithm describes the idea.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-content&quot; data-lang=&quot;content&quot;&gt;Input: users X, movies m, rating r, movie genre m g , Number of movies to be recommended(μ).

Output: Recommended movies R

1. for all users do
2. Select seen movies s, unseen movies s&#39;, association
of unseen movies as_i&#39; w.r.t X, association of each 
genre ag_j w.r.t s&#39; , where i is 1 to n and j is 1 to m.
3. Calculate score_j .
4. Select highest three score_j
5. Select m&#39; ⊂ s&#39; according to highest three score_j
6. Calculate score m_e&#39; where e ∈ m&#39; 
7. Return top μ score recommendations.
8. end for&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-conten1&quot; data-lang=&quot;conten1&quot;&gt;In this algorithm, the notations used have the following meaning :
association of each movie as_i represents total number of users who rated movie i Є s&#39;;
association of each genre ag_j represents total number of movie belonging to genre j.

score_j = ag_j / m
score(m_e&#39;) = am_e / total count of m&#39;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;2) Collaborative Filtering&lt;/p&gt;

&lt;p&gt;There can be many users who must be having the same pattern of rating an item as the user intended. This similar pattern of their ratings with the user guides the Collaborative Filtering. The notion behind the Collaborative Filtering is the recommendation of an item based on the
preferences of like-minded users.&lt;/p&gt;

&lt;p&gt;The Algorithm used for the implementation is given below :&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-collaborative&quot; data-lang=&quot;collaborative&quot;&gt;Input: users X, movies m, rating r, Number of movies to be recommended(μ).

Output: Recommended movies R.
1. for all users do
2. Select seen movies s, unseen movies s&#39;
3. Find similarity (sim_i ) w.r.t s, where i = 1 to n.
4. Select highest sim_i user
5. Select m&#39; Є s of user obtained in step 4 and s&#39; of i_th user.
6. Calculate weight W(m_e &#39;) where e Є m&#39;
7. Return top μ weight recommendations.
8. end for&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-collaborative1&quot; data-lang=&quot;collaborative1&quot;&gt;In this algorithm, the notations used have the following meaning :
sim_i represents common movies between user i and other users.

weight(m e &#39;)= rating of particular movie e / max rating.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;3) Hybrid Filtering&lt;/p&gt;

&lt;p&gt;To cater better precision, a hybrid filtering method is used which can provide the advantages of both the content and the collaborative approaches and can overcome their shortcomings. Suppose, the user appreciates mostly movies in g ⊂ G genres, and the collaborating users also give high ratings to the g ⊂ G genres, then g will be taken as the metric to recommend movies to the user.&lt;/p&gt;

&lt;p&gt;The algorithm for Hybrid Filtering has been used as :&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-hybrid&quot; data-lang=&quot;hybrid&quot;&gt;Input: users X, movies m, rating r, movie genre m g , Number of movies to be recommended(μ).

Output: Recommended movies R.

1. for all users do
2. Select seen movies s, unseen movies s&#39;, association of each genre ag_j w.r.t s&#39;, where i is 1 to n and j is 1 to m.
3. Calculate score_j .
4. Select highest three score_j
5. Select m&#39;&#39; Є s of the i_th user according to highest three score_j
6. Find similarity (sim_j ) w.r.t m&#39;&#39;
7. Select highest sim_j user.
8. Select m&#39; according to its highest three score_j Є s of user obtained in step 7 and s&#39; of the i_th user under consideration.
9. Calculate weight W(m_e&#39;) where e Є m&#39;
10. Return top μ weight recommendations.
11. end for&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;4) K-Mean Clustering&lt;/p&gt;

&lt;p&gt;The k-mean is a non parametric classification technique. It distributes the items into k clusters according to their proximity to one another. In this paper, this proximity is being measured by using the Euclidean distance. For calculating the Euclidean distance we have taken rated and unrated movies as binary. Each cluster possesses a centroid which is the mean of all the items in the cluster. All the objects in a cluster move
towards the centroid and the centroid is updated in each iteration. The iteration continues until a saturation point arrives, when the centroid stops altering. By following this approach we are decreasing the search space which results in reduced computational complexity. These computations are performed off-line which helps the classification to be efficient in terms of time complexity.&lt;/p&gt;

&lt;p&gt;The K-means clustering Algorithm is given as :&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-kmeans&quot; data-lang=&quot;kmeans&quot;&gt;Input: users X, movies m, rating r, Number of movies to be recommended μ, value of k.

Output: Recommended movie R.

1. begin
2. Randomly select k centroids.
3. Calculate euclidean distance (eucd) for X from k centroids.
4. Allocate X to k_th cluster according to eucd.
5. Update centroid for each cluster with (summation(k_i) from 1 to p)/p, where p is the number of members in k_i cluster
6. Repeat step 3 to step 5 until centroid(t) ≠ centroid (t+1).
7. for all users do
8. Select seen movies s, unseen movies s&#39;.
9. Find similarity (sim_i ) w.r.t s, where i = 1 to p.
10. Select highest sim_i user.
11. select m&#39; ⊂ s of highest sim_i and s&#39; of i_th user.
12. Calculate weight W(m_e&#39;) where e Є m&#39;
13. Return top μ weight recommendations.
14. end for
15. end&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;5) Naive Bayes&lt;/p&gt;

&lt;p&gt;The Naive Bayes is based on the Bayes theorem. The probabilistic approach followed by Naive Bayes Classifier determines the probability of the classification and helps in finding the uncertainty about the model. It is an efficient learning algorithm which uses the prior knowledge of the
observed data. The Naive assumption is that the features are conditionally independent.&lt;/p&gt;

&lt;p&gt;The algorithm used is given below :&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-kmeans&quot; data-lang=&quot;kmeans&quot;&gt;Input: users X, movies m, rating r, Number of movies to be recommended μ, value of k.

Output: Recommended movie R.

Input: users X, movies m, rating r, number of movies to be recommended(μ)

Output: Recommended movies R.

1. for all users do
2. Select seen movies s, unseen movies s&#39; .
3. Find similarity (sim_i) w.r.t s, where i = 1 to n.
4. Select x&#39;⊂ X where sim_i &amp;gt; 10.
5. Calculate association of unseen movies as i&#39; w.r.t to x&#39;
6. Calculate score (s_e &#39;) where e Є s&#39;.
7. Return top μ score recommendations.
8. end for&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We now illustrate the analysis of the experiments performed and provide a comparison of all the state-of-the-art methods described above. To compare their accuracy we have used the MovieLens dataset of 10K, 50K and 100K. The dataset varies in sparsity. For example, the 100K MovieLens dataset has 100K ratings, 943 users and 1682 movies of 19 different genres. The analysis of these algorithms is demonstrated based on precision measure. For each test user, we convert 30% of the user’s seen movies into unseen movies and apply the algorithms described above. Out of the total number of recommendations (T), the ones which are also present in the converted movies are the correct recommendations(tc).&lt;/p&gt;

&lt;p&gt;Precision = (Σ tc / Σ T) * 100
For all the experiments, we are taking value of μ = 5 and value of k = 10.&lt;/p&gt;

&lt;p&gt;The table below shows the precision of these algorithms as calculated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/precision_table.png&quot; alt=&quot;Precision&quot; title=&quot;Precision&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The results obtained can be visualized by the following figure :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/precision_diagram.png&quot; alt=&quot;PrecisionDiagram&quot; title=&quot;PrecisionDiagram&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;All the algorithms described in this paper are compared with respect to their precision rates. This comprehensive analysis depicts the strength and the weakness of each one of them in different versions of the MovieLens dataset. The experiments performed are the witness of the sparsity handling by these algorithms. Our experiments have shown promising results and this paper conforms that out of all these approaches Naive
Bayes gives the best precision.&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://localhost:4000/Machine_Learning_Algorithms_for_Recommender_System_a_comparative_analysis/&quot;&gt;Machine Learning Algorithms for Recommender System - a comparative analysis&lt;/a&gt; was originally published by Anand Nautiyal at &lt;a href=&quot;http://localhost:4000&quot;&gt;Anand's Blog&lt;/a&gt; on April 15, 2017.&lt;/p&gt;
  </content>
</entry>

</feed>