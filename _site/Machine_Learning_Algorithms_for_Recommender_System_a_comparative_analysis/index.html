<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Machine Learning Algorithms for Recommender System - a comparative analysis &#8211; Anand's Blog</title>
<meta name="description" content="An analysis of the hotshot machine learning algorithms">
<meta name="keywords" content="MachineLearning, Clustering, Classification">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Machine Learning Algorithms for Recommender System - a comparative analysis">
<meta name="twitter:description" content="An analysis of the hotshot machine learning algorithms">



<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:4000/images/Machine-Learning.jpg">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Algorithms for Recommender System - a comparative analysis">
<meta property="og:description" content="An analysis of the hotshot machine learning algorithms">
<meta property="og:url" content="http://localhost:4000/Machine_Learning_Algorithms_for_Recommender_System_a_comparative_analysis/">
<meta property="og:site_name" content="Anand's Blog">





<link rel="canonical" href="http://localhost:4000/Machine_Learning_Algorithms_for_Recommender_System_a_comparative_analysis/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Anand's Blog Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
<!-- Webfonts -->
<script src="https://use.edgefonts.net/source-sans-pro:n2,i2,n3,i3,n4,i4,n6,i6,n7,i7,n9,i9;source-code-pro:n4,n7;volkhov.js"></script>

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
  <script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
  <script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>


<!-- MathJax -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body id="post">

<div class="navigation-wrapper">
	<nav role="navigation" id="site-nav" class="animated drop">
	    <ul>
      
		    
		    <li><a href="http://localhost:4000/" >Home</a></li>
		  
		    
		    <li><a href="http://localhost:4000/portfolio/" >Portfolio</a></li>
		  
		    
		    <li><a href="http://localhost:4000/resume/" >Resume</a></li>
		  
		    
		    <li><a href="http://localhost:4000/about/" >About</a></li>
		  
	    </ul>
	</nav>
</div><!-- /.navigation-wrapper -->

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->

<header class="masthead">
	
		<div class="wrap">
			<a href="http://localhost:4000/" class="site-logo" rel="home" title="Anand's Blog"><img src="http://localhost:4000/images/Anand.jpg" width="200" height="200" alt="Anand's Blog logo" class="animated fadeInDown"></a>
		</div>
	
</header><!-- /.masthead -->


<div class="js-menu-screen menu-screen"></div>


<div id="main" role="main">
  <article class="hentry">
    <img src="http://localhost:4000/images/Machine-Learning.jpg" class="entry-feature-image" alt="Machine Learning Algorithms for Recommender System - a comparative analysis" >
    <div class="entry-wrapper">
      <header class="entry-header">
        <ul class="entry-tags">
          <li><a href="http://localhost:4000/tags/#MachineLearning" title="Pages tagged MachineLearning">MachineLearning</a></li><li><a href="http://localhost:4000/tags/#Clustering" title="Pages tagged Clustering">Clustering</a></li><li><a href="http://localhost:4000/tags/#Classification" title="Pages tagged Classification">Classification</a></li>
        </ul>
        
          <h1 class="entry-title">Machine Learning Algorithms for Recommender System - a comparative analysis</h1>
        
      </header>
      <footer class="entry-meta">
        
        
          <img src="http://localhost:4000/images/Anand.jpg" class="bio-photo" alt="Anand Nautiyal bio photo"></a>
        
        <span class="author vcard">By <span class="fn">Anand Nautiyal</span></span>
        <span class="entry-date date published"><time datetime="2017-04-15T18:54:48+05:30"><i class="fa fa-calendar-o"></i> April 15, 2017</time></span>
        
        
        
        
      </footer>
      <div class="entry-content">
        <h1 id="a-thorough-analysis-of-the-machine-learning-algorithms">A thorough analysis of the Machine Learning Algorithms</h1>

<h3 id="table-of-contents">Table of Contents</h3>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#requirements">Requirements</a></li>
  <li><a href="#algorithms">Algorithms</a></li>
  <li><a href="#results">Results</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>

<p>Recommendation system is one of the most popular applications of Artificial Intelligence which attracts many researchers all over the globe. The advent of the Internet era has brought wide implementation of recommendation system in our everyday lives. There are many machine learning techniques which can be used to realize the recommendation system. Among all these techniques we are dealing with Content Based Filtering, Collaborative Based Filtering, Hybrid Content-Collaborative Based Filtering, k-mean clustering and Naive Bayes classifier. We have exploited these algorithms to their extreme in order to achieve the best possible precision and have presented a comprehensive comparative analysis. The strength of all these algorithms can be clearly realized by the significant enhancement in the accuracy, depicted by the experimental analysis taking cold start problem into consideration.</p>

<h2 id="requirements">Requirements</h2>

<ul>
  <li>Netbeans (8.1)</li>
  <li>Java (1.8)</li>
  <li>RAM (&gt;= 4 GB)</li>
  <li>Processor (&gt;= 2 Ghz)</li>
</ul>

<h3 id="algorithms">Algorithms</h3>

<p>1) Content Based Filtering</p>

<p>The Content Based Filtering considers the items rated by a user to formulate the future recommendations while exploring the internet services. A user tends to rate an item which he likes or dislikes. His ratings reflect his response towards that item. If he likes an item, he rates it higher and lesser ratings denote that he is not much interested. These rated items serve as the ‘content’ in the Content Based Filtering. Based on this content, the user is recommended future items which he might approve of. Here, the user is recommended movies which fall in a particular
genre of his liking.</p>

<p>Following Algorithm describes the idea.</p>

<figure class="highlight"><pre><code class="language-content" data-lang="content">Input: users X, movies m, rating r, movie genre m g , Number of movies to be recommended(μ).

Output: Recommended movies R

1. for all users do
2. Select seen movies s, unseen movies s', association
of unseen movies as_i' w.r.t X, association of each 
genre ag_j w.r.t s' , where i is 1 to n and j is 1 to m.
3. Calculate score_j .
4. Select highest three score_j
5. Select m' ⊂ s' according to highest three score_j
6. Calculate score m_e' where e ∈ m' 
7. Return top μ score recommendations.
8. end for</code></pre></figure>

<figure class="highlight"><pre><code class="language-conten1" data-lang="conten1">In this algorithm, the notations used have the following meaning :
association of each movie as_i represents total number of users who rated movie i Є s';
association of each genre ag_j represents total number of movie belonging to genre j.

score_j = ag_j / m
score(m_e') = am_e / total count of m'</code></pre></figure>

<p>2) Collaborative Filtering</p>

<p>There can be many users who must be having the same pattern of rating an item as the user intended. This similar pattern of their ratings with the user guides the Collaborative Filtering. The notion behind the Collaborative Filtering is the recommendation of an item based on the
preferences of like-minded users.</p>

<p>The Algorithm used for the implementation is given below :</p>

<figure class="highlight"><pre><code class="language-collaborative" data-lang="collaborative">Input: users X, movies m, rating r, Number of movies to be recommended(μ).

Output: Recommended movies R.
1. for all users do
2. Select seen movies s, unseen movies s'
3. Find similarity (sim_i ) w.r.t s, where i = 1 to n.
4. Select highest sim_i user
5. Select m' Є s of user obtained in step 4 and s' of i_th user.
6. Calculate weight W(m_e ') where e Є m'
7. Return top μ weight recommendations.
8. end for</code></pre></figure>

<figure class="highlight"><pre><code class="language-collaborative1" data-lang="collaborative1">In this algorithm, the notations used have the following meaning :
sim_i represents common movies between user i and other users.

weight(m e ')= rating of particular movie e / max rating.</code></pre></figure>

<p>3) Hybrid Filtering</p>

<p>To cater better precision, a hybrid filtering method is used which can provide the advantages of both the content and the collaborative approaches and can overcome their shortcomings. Suppose, the user appreciates mostly movies in g ⊂ G genres, and the collaborating users also give high ratings to the g ⊂ G genres, then g will be taken as the metric to recommend movies to the user.</p>

<p>The algorithm for Hybrid Filtering has been used as :</p>

<figure class="highlight"><pre><code class="language-hybrid" data-lang="hybrid">Input: users X, movies m, rating r, movie genre m g , Number of movies to be recommended(μ).

Output: Recommended movies R.

1. for all users do
2. Select seen movies s, unseen movies s', association of each genre ag_j w.r.t s', where i is 1 to n and j is 1 to m.
3. Calculate score_j .
4. Select highest three score_j
5. Select m'' Є s of the i_th user according to highest three score_j
6. Find similarity (sim_j ) w.r.t m''
7. Select highest sim_j user.
8. Select m' according to its highest three score_j Є s of user obtained in step 7 and s' of the i_th user under consideration.
9. Calculate weight W(m_e') where e Є m'
10. Return top μ weight recommendations.
11. end for</code></pre></figure>

<p>4) K-Mean Clustering</p>

<p>The k-mean is a non parametric classification technique. It distributes the items into k clusters according to their proximity to one another. In this paper, this proximity is being measured by using the Euclidean distance. For calculating the Euclidean distance we have taken rated and unrated movies as binary. Each cluster possesses a centroid which is the mean of all the items in the cluster. All the objects in a cluster move
towards the centroid and the centroid is updated in each iteration. The iteration continues until a saturation point arrives, when the centroid stops altering. By following this approach we are decreasing the search space which results in reduced computational complexity. These computations are performed off-line which helps the classification to be efficient in terms of time complexity.</p>

<p>The K-means clustering Algorithm is given as :</p>

<figure class="highlight"><pre><code class="language-kmeans" data-lang="kmeans">Input: users X, movies m, rating r, Number of movies to be recommended μ, value of k.

Output: Recommended movie R.

1. begin
2. Randomly select k centroids.
3. Calculate euclidean distance (eucd) for X from k centroids.
4. Allocate X to k_th cluster according to eucd.
5. Update centroid for each cluster with (summation(k_i) from 1 to p)/p, where p is the number of members in k_i cluster
6. Repeat step 3 to step 5 until centroid(t) ≠ centroid (t+1).
7. for all users do
8. Select seen movies s, unseen movies s'.
9. Find similarity (sim_i ) w.r.t s, where i = 1 to p.
10. Select highest sim_i user.
11. select m' ⊂ s of highest sim_i and s' of i_th user.
12. Calculate weight W(m_e') where e Є m'
13. Return top μ weight recommendations.
14. end for
15. end</code></pre></figure>

<p>5) Naive Bayes</p>

<p>The Naive Bayes is based on the Bayes theorem. The probabilistic approach followed by Naive Bayes Classifier determines the probability of the classification and helps in finding the uncertainty about the model. It is an efficient learning algorithm which uses the prior knowledge of the
observed data. The Naive assumption is that the features are conditionally independent.</p>

<p>The algorithm used is given below :</p>

<figure class="highlight"><pre><code class="language-kmeans" data-lang="kmeans">Input: users X, movies m, rating r, Number of movies to be recommended μ, value of k.

Output: Recommended movie R.

Input: users X, movies m, rating r, number of movies to be recommended(μ)

Output: Recommended movies R.

1. for all users do
2. Select seen movies s, unseen movies s' .
3. Find similarity (sim_i) w.r.t s, where i = 1 to n.
4. Select x'⊂ X where sim_i &gt; 10.
5. Calculate association of unseen movies as i' w.r.t to x'
6. Calculate score (s_e ') where e Є s'.
7. Return top μ score recommendations.
8. end for</code></pre></figure>

<h2 id="results">Results</h2>

<p>We now illustrate the analysis of the experiments performed and provide a comparison of all the state-of-the-art methods described above. To compare their accuracy we have used the MovieLens dataset of 10K, 50K and 100K. The dataset varies in sparsity. For example, the 100K MovieLens dataset has 100K ratings, 943 users and 1682 movies of 19 different genres. The analysis of these algorithms is demonstrated based on precision measure. For each test user, we convert 30% of the user’s seen movies into unseen movies and apply the algorithms described above. Out of the total number of recommendations (T), the ones which are also present in the converted movies are the correct recommendations(tc).</p>

<p>Precision = (Σ tc / Σ T) * 100
For all the experiments, we are taking value of μ = 5 and value of k = 10.</p>

<p>The table below shows the precision of these algorithms as calculated.</p>

<p><img src="/precision_table.png" alt="Precision" title="Precision" /></p>

<p>The results obtained can be visualized by the following figure :</p>

<p><img src="/precision_diagram.png" alt="PrecisionDiagram" title="PrecisionDiagram" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>All the algorithms described in this paper are compared with respect to their precision rates. This comprehensive analysis depicts the strength and the weakness of each one of them in different versions of the MovieLens dataset. The experiments performed are the witness of the sparsity handling by these algorithms. Our experiments have shown promising results and this paper conforms that out of all these approaches Naive
Bayes gives the best precision.</p>


        
      </div><!-- /.entry-content -->
    </div><!-- /.entry-wrapper -->
    <nav class="pagination" role="navigation">
      
      
        <a href="http://localhost:4000/blog/Time_efficient_Disvovery_of_-moving_object_groups_from_Trajectory_data/" class="btn" title="Time efficient Disvovery of moving object groups from Trajectory data">Next</a>
      
    </nav><!-- /.pagination -->
  </article>
</div><!-- /#main -->



<div class="footer-wrapper">
  <footer role="contentinfo" class="entry-wrapper">
    

<span><img src="https://mirrors.creativecommons.org/presskit/icons/cc.large.png" style="width:20px;height:20px;"> 2018  Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll.</a> </span>
<div class="social-icons">
	
	<a href="https://facebook.com/anandnautiyal123" title="Anand Nautiyal on Facebook" target="_blank"><i class="fa fa-facebook-square fa-2x"></i></a>
	
	<a href="https://linkedin.com/in/anand-nautiyal-76227b7/" title="Anand Nautiyal on LinkedIn" target="_blank"><i class="fa fa-linkedin-square fa-2x"></i></a>
	
	<a href="https://instagram.com/anand_23" title="Anand Nautiyal on Instagram" target="_blank"><i class="fa fa-instagram fa-2x"></i></a>
	
	<a href="https://github.com/AnandNautiyal23" title="Anand Nautiyal on Github" target="_blank"><i class="fa fa-github-square fa-2x"></i></a>
	
  
	
  <a href="http://localhost:4000/feed.xml" title="Atom/RSS feed"><i class="fa fa-rss-square fa-2x"></i></a>
</div><!-- /.social-icons -->

  </footer>
</div><!-- /.footer-wrapper -->

<script type="text/javascript">
  var BASE_URL = 'http://localhost:4000';
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>




</body>
</html>
